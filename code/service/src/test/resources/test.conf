jdbc-default {
	schema = "cos"
}
akka {
	loglevel = ERROR
	log-dead-letters-during-shutdown = on
	log-dead-letters = on
	actor {
		serialize-messages = off
		debug {
			receive = on // log all messages sent to an actor if that actors receive method is a LoggingReceive
			autoreceive = on // log all special messages like Kill, PoisoffPill etc sent to all actors
			lifecycle = on // log all actor lifecycle events of all actors
			fsm = off // enable logging of all events, transitioffs and timers of FSM Actors that extend LoggingFSM
			event-stream = on // enable logging of subscriptions (subscribe/unsubscribe) on the ActorSystem.eventStream
		}
		serializers {
			cosSerializer = "com.namely.chiefofstate.serialization.CosSerializer"
			proto = "akka.remote.serialization.ProtobufSerializer"
		}
		serialization-bindings {
			"scalapb.GeneratedMessage" = cosSerializer
			"com.namely.chiefofstate.serialization.ScalaMessage" = cosSerializer
			"com.google.protobuf.Message" = proto
		}

		# This will stop the guardian actor in case of any exception which will therefore
		# shutdown the whole actor system
		guardian-supervisor-strategy = "akka.actor.StoppingSupervisorStrategy"
	}

	persistence {
		# akka persistence jdbc
		journal.plugin = "jdbc-journal"
		snapshot-store.plugin = "jdbc-snapshot-store"
	}

	coordinated-shutdown {
		terminate-actor-system = off
		exit-jvm = off
		run-by-actor-system-terminate = off
		run-by-jvm-shutdown-hook = off
	}

	projection {
		jdbc {
			# choose one of: mysql-dialect, postgres-dialect, mssql-dialect, oracle-dialect or h2-dialect (testing)
			dialect = "postgres-dialect"
			blocking-jdbc-dispatcher {
				type = Dispatcher
				executor = "thread-pool-executor"
				thread-pool-executor {
					fixed-pool-size = 1
				}
				throughput = 1
			}

			offset-store {
				# set this to your database schema if applicable, empty by default
				schema = ${jdbc-default.schema}
				# the database table name for the offset store
				table = "read_side_offsets"
				# Use lowercase table and column names.
				use-lowercase-schema = true
			}
			debug.verbose-offset-store-logging = false
		}

		restart-backoff {
			min-backoff = 3s
			max-backoff = 30s
			random-factor = 0.2
			max-restarts = -1
		}
		recovery-strategy {
			strategy = fail
			retries = 5
			retry-delay = 1 s
		}
	}
}

# general slick configuration
write-side-slick {
	profile = "slick.jdbc.PostgresProfile$"
	db {
		connectionPool = disabled
		driver = "org.postgresql.Driver"
		user = ""
		password = ""
		serverName = ""
		databaseName = ""
		url = ""
	}
}

jdbc-journal {
	tables {
		# Only used in pre 5.0.0 for backward-compatibility
		# ref: https://github.com/akka/akka-persistence-jdbc/blob/v5.0.0/core/src/main/resources/reference.conf
		legacy_journal {
			tableName = "journal"
			schemaName = ${jdbc-default.schema}
			columnNames {
				ordering = "ordering"
				deleted = "deleted"
				persistenceId = "persistence_id"
				sequenceNumber = "sequence_number"
				created = "created"
				tags = "tags"
				message = "message"
			}
		}

		# this is the new going forward
		# ref: https://github.com/akka/akka-persistence-jdbc/blob/v5.0.0/core/src/main/resources/reference.conf
		event_journal {
			tableName = "event_journal"
			schemaName = ${jdbc-default.schema}
			columnNames {
				ordering = "ordering"
				deleted = "deleted"
				persistenceId = "persistence_id"
				sequenceNumber = "sequence_number"
				writer = "writer"
				writeTimestamp = "write_timestamp"
				adapterManifest = "adapter_manifest"
				eventPayload = "event_payload"
				eventSerId = "event_ser_id"
				eventSerManifest = "event_ser_manifest"
				metaPayload = "meta_payload"
				metaSerId = "meta_ser_id"
				metaSerManifest = "meta_ser_manifest"
			}
		}

		event_tag {
			tableName = "event_tag"
			schemaName = ${jdbc-default.schema}
			columnNames {
				eventId = "event_id"
				tag = "tag"
			}
		}
	}
	tagSeparator = ","
	bufferSize = 1000
	batchSize = 400
	replayBatchSize = 400
	parallelism = 8
	logicalDelete = true
	dao = "akka.persistence.jdbc.journal.dao.DefaultJournalDao"
	slick = ${write-side-slick}
}

# the akka-persistence-query provider in use
jdbc-read-journal {
	# New events are retrieved (polled) with this interval.
	refresh-interval = "1s"
	# How many events to fetch in one query (replay) and keep buffered until they
	# are delivered downstreams.
	max-buffer-size = "500"
	tables {
		legacy_journal = ${jdbc-journal.tables.legacy_journal}
		event_journal = ${jdbc-journal.tables.event_journal}
		event_tag = ${jdbc-journal.tables.event_tag}
	}

	tagSeparator = ","
	# if true, queries will include logically deleted events
	# should not be configured directly, but through property akka-persistence-jdbc.logicalDelete.enable
	# in order to keep consistent behavior over write/read sides
	includeLogicallyDeleted = true

	# Settings for determining if ids (ordering column) in the journal are out of sequence.
	journal-sequence-retrieval {
		# The maximum number of ids that will be retrieved in each batch
		batch-size = 10000
		# In case a number in the sequence is missing, this is the ammount of retries that will be done to see
		# if the number is still found. Note that the time after which a number in the sequence is assumed missing is
		# equal to maxTries * queryDelay
		# (maxTries may not be zero)
		max-tries = 10
		# How often the actor will query for new data
		query-delay = 1 second
		# The maximum backoff time before trying to query again in case of database failures
		max-backoff-query-delay = 1 minute
		# The ask timeout to use when querying the journal sequence actor, the actor should normally repond very quickly,
		# since it always replies with its current internal state
		ask-timeout = 1 second
	}

	dao = "akka.persistence.jdbc.query.dao.DefaultReadJournalDao"
	slick = ${write-side-slick}
}

# the akka-persistence-snapshot-store in use
jdbc-snapshot-store {
	tables {
		# Only used in pre 5.0.0 for backward-compatibility
		# ref: https://github.com/akka/akka-persistence-jdbc/blob/v5.0.0/core/src/main/resources/reference.conf
		legacy_snapshot {
			tableName = "snapshot"
			schemaName = ${jdbc-default.schema}

			columnNames {
				persistenceId = "persistence_id"
				sequenceNumber = "sequence_number"
				created = "created"
				snapshot = "snapshot"
			}
		}

		# This is the new configuration going forward
		snapshot {
			tableName = "state_snapshot"
			schemaName = ${jdbc-default.schema}
			columnNames {
				persistenceId = "persistence_id"
				sequenceNumber = "sequence_number"
				created = "created"

				snapshotPayload = "snapshot_payload"
				snapshotSerId = "snapshot_ser_id"
				snapshotSerManifest = "snapshot_ser_manifest"

				metaPayload = "meta_payload"
				metaSerId = "meta_ser_id"
				metaSerManifest = "meta_ser_manifest"
			}
		}
	}
	dao = "akka.persistence.jdbc.snapshot.dao.DefaultSnapshotDao"

	slick = ${write-side-slick}
}

chiefofstate {
	# the service name
	service-name = "chiefofstate"

	# Ask timeout is required to
	# send commands to the aggregate root and receive response
	# the unit value is in second
	ask-timeout = 5

	snapshot-criteria {
		# Snapshots are not saved and deleted automatically, events are not deleted
		disable-snapshot = false
		# Save snapshots automatically every `retention-frequency`
		# Snapshots that have sequence number less than sequence number of the saved snapshot minus `retention-number * retention-frequency` are
		retention-frequency = 100
		# number of snapshots to retain
		retention-number = 2
		# this feature allow to clean the journal history
		# Event deletion is triggered after saving a new snapshot. Old events would be deleted prior to old snapshots being deleted
		# reference: https://doc.akka.io/docs/akka/current/typed/persistence-snapshot.html#event-deletion
		delete-events-on-snapshot = false
	}

	grpc {
		client {
			# the deadline timeout, a duration of time after which the RPC times out.
			# it is expressed in milliseconds and it should be less than chiefofstate.ask-timeout
			# Fail to accomplish this will result in timeout exception
			# The default values is 1mn which is really lenient
			deadline-timeout = 60000
		}
		server {
			port = 9000
			address = "0.0.0.0"
		}
	}

	write-side {
		# Write handler gRPC host
		host = ""
		# Write handler gRPC port
		port = -1
		# enable TLS on outbound write-handler gRPC calls
		use-tls = false
		# Whether to allow validation of the state and events FQNs
		# if set to true, validation is done, by default it is false.
		enable-protos-validation = false
		# define the fully qualified type url  of the state proto
		# example: chief_of_state.v1.Account
		states-protos = ""
		# list if the fully qualified type url  of the events handled
		# example: "chief_of_state.v1.AccountOpened", "chief_of_state.v1.AccountDebited"
		events-protos = ""
		# csv of gRPC headers to propagate to write-side handler
		propagated-headers = ""
		# csv of gRPC headers to persist to journal
		persisted-headers = ""
	}

	read-side {
		# set this value to true whenever a readSide config is set
		enabled = false
	}

	plugin-settings {
		enabled-plugins = ""
	}

	telemetry {
		namespace = ""
		otlp_endpoint = ""
		trace_propagators = "b3multi"
	}

	# migration configuration
	# This setting are required for a smooth migration to 0.8.0
	migration {
		v1 {
			slick {
				profile = "slick.jdbc.PostgresProfile$"
				# add here your Slick db settings
				db {
					driver = "org.postgresql.Driver"
					user = ${write-side-slick.db.user}
					password = ${write-side-slick.db.password}
					serverName = ${write-side-slick.db.serverName}
					databaseName = ${write-side-slick.db.databaseName}
					url = ${write-side-slick.db.url}
					connectionPool = "HikariCP"
					keepAliveConnection = false
					# See some tips on thread/connection pool sizing on https://github.com/brettwooldridge/HikariCP/wiki/About-Pool-Sizing
					# Keep in mind that the number of threads must equal the maximum number of connections.
					numThreads = 5
					maxConnections = 5
					minConnections = 0
					idleTimeout = 60000 # 60 seconds
					maxLifetime = 120000 # 120 seconds
				}

				offset-store {
					schema = ${jdbc-default.schema}
					table = "read_side_offsets"
					use-lowercase-schema = false
				}
			}
		}
	}
}
