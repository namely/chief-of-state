akka {
	loglevel = DEBUG
	loglevel = ${?LOG_LEVEL}
	log-dead-letters-during-shutdown = on
	log-dead-letters = on
	actor {
		serialize-messages = off
		debug {
			receive = on // log all messages sent to an actor if that actors receive method is a LoggingReceive
			autoreceive = on // log all special messages like Kill, PoisoffPill etc sent to all actors
			lifecycle = on // log all actor lifecycle events of all actors
			fsm = off // enable logging of all events, transitioffs and timers of FSM Actors that extend LoggingFSM
			event-stream = on // enable logging of subscriptions (subscribe/unsubscribe) on the ActorSystem.eventStream
		}
		provider = cluster
		serializers {
			proto = "akka.remote.serialization.ProtobufSerializer"
			cmdSerializer = "com.namely.chiefofstate.CommandSerializer"
		}
		serialization-bindings {
			# state is serialized using protobuf
			"scalapb.GeneratedMessage" = proto
			"com.namely.chiefofstate.AggregateCommand" = cmdSerializer
		}
	}
	persistence {
		# akka persistence jdbc
		journal.plugin = "jdbc-journal"
		snapshot-store.plugin = "jdbc-snapshot-store"
	}
	cluster {
		sharding {
			# Number of shards used by the default HashCodeMessageExtractor
			# when no other message extractor is defined. This value must be
			# the same for all nodes in the cluster and that is verified by
			# configuration check when joining. Changing the value requires
			# stopping all nodes in the cluster.
			number-of-shards = 10
			number-of-shards = ${?COS_NUM_SHARDS}
			# Set this to a time duration to have sharding passivate entities when they have not
			# received any message in this length of time. Set to 'off' to disable.
			# It is always disabled if `remember-entities` is enabled.
			passivate-idle-entity-after = 120s
		}
		downing-provider-class = "akka.cluster.sbr.SplitBrainResolverProvider"
		# Documentation: https://doc.akka.io/docs/akka/current/split-brain-resolver.html#split-brain-resolver
		downing-provider-class = "akka.cluster.sbr.SplitBrainResolverProvider"
		split-brain-resolver {
			# Select one of the available strategies (see descriptions below):
			# static-quorum, keep-majority, keep-oldest, down-all, lease-majority
			# in a Kubernetes environment the Lease strategy can be a good choice.
			active-strategy = keep-majority
			# Time margin after which shards or singletons that belonged to a downed/removed
			# partition are created in surviving partition. The purpose of this margin is that
			# in case of a network partition the persistent actors in the non-surviving partitions
			# must be stopped before corresponding persistent actors are started somewhere else.
			# This is useful if you implement downing strategies that handle network partitions,
			# e.g. by keeping the larger side of the partition and shutting down the smaller side.
			# Decision is taken by the strategy when there has been no membership or
			# reachability changes for this duration, i.e. the cluster state is stable.
			stable-after = 20s
			# When reachability observations by the failure detector are changed the SBR decisions
			# are deferred until there are no changes within the 'stable-after' duration.
			# If this continues for too long it might be an indication of an unstable system/network
			# and it could result in delayed or conflicting decisions on separate sides of a network
			# partition.
			# As a precaution for that scenario all nodes are downed if no decision is made within
			# `stable-after + down-all-when-unstable` from the first unreachability event.
			# The measurement is reset if all unreachable have been healed, downed or removed, or
			# if there are no changes within `stable-after * 2`.
			# The value can be on, off, or a duration.
			# By default it is 'on' and then it is derived to be 3/4 of stable-after.
			down-all-when-unstable = on
		}
		shutdown-after-unsuccessful-join-seed-nodes = 20s
	}
	coordinated-shutdown {
		terminate-actor-system = on
		exit-jvm = on
	}
	projection {
		slick {
			# The Slick profile to use
			# set to one of: slick.jdbc.DerbyProfile$, slick.jdbc.H2Profile$, slick.jdbc.HsqldbProfile$, slick.jdbc.MySQLProfile$,
			#                slick.jdbc.PostgresProfile$, slick.jdbc.SQLiteProfile$, slick.jdbc.OracleProfile$
			#profile = <fill this with your profile of choice>
			profile = "slick.jdbc.PostgresProfile$"
			# add here your Slick db settings
			db {
				driver = "org.postgresql.Driver"
				user = "postgres"
				user = ${?COS_READ_SIDE_OFFSET_DB_USER}
				password = "changeme"
				password = ${?COS_READ_SIDE_OFFSET_DB_PASSWORD}
				serverName = "localhost"
				serverName = ${?COS_READ_SIDE_OFFSET_DB_HOST}
				portNumber = 5432
				portNumber = ${?COS_READ_SIDE_OFFSET_DB_PORT}
				databaseName = "postgres"
				databaseName = ${?COS_READ_SIDE_OFFSET_DB}
				url = "jdbc:postgresql://"${akka.projection.slick.db.serverName}":"${akka.projection.slick.db.portNumber}"/"${akka.projection.slick.db.databaseName}"?currentSchema="${akka.projection.slick.offset-store.schema}
				connectionPool = "HikariCP"
				keepAliveConnection = true
			}

			offset-store {
				# set this to your database schema if applicable, empty by default
				schema = "public"
				schema = ${?COS_READ_SIDE_OFFSET_DB_SCHEMA}
				# the database table name for the offset store
				table = "AKKA_PROJECTION_OFFSET_STORE"
				table = ${?COS_READ_SIDE_OFFSET_STORE_TABLE}
			}
		}
		restart-backoff {
			min-backoff = 3s
			max-backoff = 30s
			random-factor = 0.2

			# -1 will not cap the amount of restarts
			# 0 will disable restarts
			max-restarts = -1
		}
	}
}

# general slick configuration
slick {
	# The Slick profile to use
	# set to one of: slick.jdbc.DerbyProfile$, slick.jdbc.H2Profile$, slick.jdbc.HsqldbProfile$, slick.jdbc.MySQLProfile$,
	#                slick.jdbc.PostgresProfile$, slick.jdbc.SQLiteProfile$, slick.jdbc.OracleProfile$
	#profile = <fill this with your profile of choice>
	profile = "slick.jdbc.PostgresProfile$"
	# add here your Slick db settings
	db {
		connectionPool = "HikariCP"
		driver = "org.postgresql.Driver"
		user = "postgres"
		user = ${?COS_POSTGRES_USER}
		password = "changeme"
		password = ${?COS_POSTGRES_PASSWORD}
		serverName = "localhost"
		serverName = ${?COS_POSTGRES_HOST}
		portNumber = 5432
		portNumber = ${?COS_POSTGRES_PORT}
		databaseName = "postgres"
		databaseName = ${?COS_POSTGRES_DB}
		url = "jdbc:postgresql://"${slick.db.serverName}":"${slick.db.portNumber}"/"${slick.db.databaseName}"?currentSchema="${jdbc-journal.tables.journal.schemaName}
		# hikariCP settings; see: https://github.com/brettwooldridge/HikariCP
		# Slick will use an async executor with a fixed size queue of 10.000 objects
		# The async executor is a connection pool for asynchronous execution of blocking I/O actions.
		# This is used for the asynchronous query execution API on top of blocking back-ends like JDBC.
		queueSize = 10000 // number of objects that can be queued by the async exector
		# ensures that the database does not get dropped while we are using it
		keepAliveConnection = on
		# See some tips on thread/connection pool sizing on https://github.com/brettwooldridge/HikariCP/wiki/About-Pool-Sizing
		# Keep in mind that the number of threads must equal the maximum number of connections.
		numThreads = 20
		maxConnections = 20
		minConnections = 20
	}
}

jdbc-journal {
	tables {
		journal {
			tableName = "journal"
			schemaName = "public"
			schemaName = ${?COS_POSTGRES_SCHEMA}
		}
	}
	slick = ${slick}
}
# the akka-persistence-query provider in use
jdbc-read-journal {
	# New events are retrieved (polled) with this interval.
	refresh-interval = "1s"
	# How many events to fetch in one query (replay) and keep buffered until they
	# are delivered downstreams.
	max-buffer-size = "500"
	tables {
		journal {
			tableName = "journal"
			schemaName = "public"
			schemaName = ${?COS_POSTGRES_SCHEMA}
		}
	}
	slick = ${slick}
}
# the akka-persistence-snapshot-store in use
jdbc-snapshot-store {
	tables {
		snapshot {
			tableName = "snapshot"
			schemaName = "public"
			schemaName = ${?COS_POSTGRES_SCHEMA}
		}
	}
	slick = ${slick}
}
# chief of state configuration
chiefofstate {
	# the service name
	service-name = "chiefofstate"
	service-name = ${?COS_SERVICE_NAME}

	# Ask timeout is required to
	# send commands to the aggregate root and receive response
	# the unit value is in second
	ask-timeout = 5
	ask-timeout = ${?COS_COMMAND_HANDLER_TIMEOUT}

	snapshot-criteria {
		#  Snapshots are not saved and deleted automatically, events are not deleted
		disable-snapshot = false
		disable-snapshot = ${?COS_DISABLE_SNAPSHOT}
		# number of events to batch persist
		retention-frequency = 100
		retention-frequency = ${?COS_EVENTS_BATCH_THRESHOLD}
		# number of snapshots to retain
		retention-number = 2
		retention-number = ${?COS_NUM_SNAPSHOTS_TO_RETAIN}
		# this feature allow to clean the journal history
		# Event deletion is triggered after saving a new snapshot. Old events would be deleted prior to old snapshots being deleted
		# reference: https://doc.akka.io/docs/akka/current/typed/persistence-snapshot.html#event-deletion
		delete-events-on-snapshot = false
		delete-events-on-snapshot = ${?COS_JOURNAL_LOGICAL_DELETION}
	}

	events {
		# the events tag name. It is recommended to use the service name
		# because the event tag name must be unique and cannot be changed once the application has handled
		# an aggregate event.
		tagname: ${chiefofstate.service-name}
		tagname: ${?COS_EVENTS_TAG_NAME}
	}

	grpc {
		client {
			# the deadline timeout, a duration of time after which the RPC times out.
			# it is expressed in milliseconds and it should be less than chiefofstate.ask-timeout
			# Fail to accomplish this will result in timeout exception
			deadline-timeout = 1000
			deadline-timeout = ${?GRPC_CALLS_TIMEOUT}
		}
		server {
			port = 9000
			port = ${?COS_PORT}
			address = "0.0.0.0"
			address = ${?COS_ADDRESS}
		}
	}

	write-side {
		# Write handler gRPC host
		host = ""
		host = ${?WRITE_SIDE_HANDLER_SERVICE_HOST}
		# Write handler gRPC port
		port = -1
		port = ${?WRITE_SIDE_HANDLER_SERVICE_PORT}
		# Whether to allow validation of the state and events FQNs
		# if set to true, validation is done, by default it is false.
		enable-protos-validation = false
		enable-protos-validation = ${?HANDLER_SERVICE_ENABLE_PROTO_VALIDATION}
		# define the fully qualified type url  of the state proto
		# example: chief_of_state.v1.Account
		states-protos = ""
		states-protos = ${?HANDLER_SERVICE_STATES_PROTOS}
		# list if the fully qualified type url  of the events handled
		# example: "chief_of_state.v1.AccountOpened", "chief_of_state.v1.AccountDebited"
		events-protos = ""
		events-protos = ${?HANDLER_SERVICE_EVENTS_PROTOS}
		# csv of gRPC headers to propagate to write-side handler
		propagated-headers = ""
		propagated-headers = ${?COS_WRITE_PROPAGATED_HEADERS}
	}

	read-side {
		# set this value to true whenever a readSide config is set
		enabled = false
		enabled = ${?COS_READ_SIDE_ENABLED}
	}

	plugin-settings {
		enabled-plugins = "com.namely.chiefofstate.plugin.PersistedHeaders"
		enabled-plugins = ${?COS_ENABLED_PLUGINS}
	}

	# persistence and read side offset store auto creation
	create-stores {
		auto = false
		auto = ${?COS_STORES_AUTO_CREATE}
	}
}

kamon {
	environment {
		service = ${chiefofstate.service-name}
	}

	trace {
		tick-interval = 1 millisecond
		sampler = always
	}

	propagation.http.default.tags.mappings {
		request-id = "x-request-id"
	}

	jaeger {
		host = "localhost"
		host = ${?JAEGER_HOST}
		port = 14268
		port = ${?JAEGER_PORT}
		http-url = ${kamon.jaeger.protocol}"://"${kamon.jaeger.host}":"${kamon.jaeger.port}"/api/traces"
		http-url = ${?JAEGER_URL}
	}
	zipkin {
		host = "localhost"
		host = ${?ZIPKIN_HOST}
		port = 9411
		port = ${?ZIPKIN_PORT}
		url = ${kamon.zipkin.protocol}"://"${kamon.zipkin.host}":"${kamon.zipkin.port}"/api/v2/spans"
		url = ${?ZIPKIN_URL}
	}
	modules {
		jaeger {
			enabled = false
			enabled = ${?JAEGER_ENABLED}
		}
		zipkin-reporter {
			enabled = false
			enabled = ${?ZIPKIN_ENABLED}
		}
	}
}
