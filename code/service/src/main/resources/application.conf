jdbc-default {
	host = "localhost"
	host = ${?COS_DB_HOST}
	database = "postgres"
	database = ${?COS_DB_NAME}
	schema = "public"
	schema = ${?COS_DB_SCHEMA}
	port = 5432
	port = ${?COS_DB_PORT}
	user = "postgres"
	user = ${?COS_DB_USER}
	password = "changeme"
	password = ${?COS_DB_PASSWORD}

	url = "jdbc:postgresql://"${jdbc-default.host}":"${jdbc-default.port}"/"${jdbc-default.database}"?currentSchema="${jdbc-default.schema}

	hikari-settings {
		#FIXME - add env vars for all these settings
		max-pool-size = 10
		min-idle-connections = 3
		idle-timeout-ms = 30000 # 30 seconds
		max-lifetime-ms = 60000 # 60 seconds
	}
}

akka {
	# this allows our application to use the old akka proto
	# serializer to read the journal pre-migration
	serialization.protobuf.allowed-classes = [
		"com.namely.protobuf.chiefofstate.v1.persistence.EventWrapper",
		"com.namely.protobuf.chiefofstate.v1.persistence.StateWrapper"
	]

	loglevel = ERROR
	loglevel = ${?LOG_LEVEL}
	log-dead-letters-during-shutdown = on
	log-dead-letters = on

	actor {
		serialize-messages = off
		debug {
			receive = on // log all messages sent to an actor if that actors receive method is a LoggingReceive
			autoreceive = on // log all special messages like Kill, PoisoffPill etc sent to all actors
			lifecycle = on // log all actor lifecycle events of all actors
			fsm = off // enable logging of all events, transitioffs and timers of FSM Actors that extend LoggingFSM
			event-stream = on // enable logging of subscriptions (subscribe/unsubscribe) on the ActorSystem.eventStream
		}
		provider = cluster
		serializers {
			cosSerializer = "com.namely.chiefofstate.serialization.CosSerializer"
			proto = "akka.remote.serialization.ProtobufSerializer"
		}
		serialization-bindings {
			"scalapb.GeneratedMessage" = cosSerializer
			"com.namely.chiefofstate.serialization.ScalaMessage" = cosSerializer
			"com.google.protobuf.Message" = proto
		}

		# This will stop the guardian actor in case of any exception which will therefore
		# shutdown the whole actor system
		guardian-supervisor-strategy = "akka.actor.StoppingSupervisorStrategy"
	}

	extensions = [akka.persistence.Persistence]

	persistence {
		# akka persistence jdbc
		journal {
			plugin = "jdbc-journal"
			# automatically starts the journal when the actor system is up
			# ref: https://doc.akka.io/docs/akka/current/persistence-plugins.html#eager-initialization-of-persistence-plugin
			# ref: https://doc.akka.io/docs/akka-persistence-jdbc/5.0.0/configuration.html#reference-configuration
			auto-start-journals = ["jdbc-journal"]
		}

		snapshot-store {
			plugin = "jdbc-snapshot-store"
			# automatically starts the snapshot-store when the actor system is up
			# ref: https://doc.akka.io/docs/akka/current/persistence-plugins.html#eager-initialization-of-persistence-plugin
			# ref: https://doc.akka.io/docs/akka-persistence-jdbc/current/configuration.html#reference-configuration
			auto-start-snapshot-stores = ["jdbc-snapshot-store"]
		}
	}

	cluster {
		sharding {
			# Number of shards used by the default HashCodeMessageExtractor
			# when no other message extractor is defined. This value must be
			# the same for all nodes in the cluster and that is verified by
			# configuration check when joining. Changing the value requires
			# stopping all nodes in the cluster.
			number-of-shards = 9
			number-of-shards = ${?COS_NUM_SHARDS}
			# Set this to a time duration to have sharding passivate entities when they have not
			# received any message in this length of time. Set to 'off' to disable.
			# It is always disabled if `remember-entities` is enabled.
			passivate-idle-entity-after = 120s
		}
		# Documentation: https://doc.akka.io/docs/akka/current/split-brain-resolver.html#split-brain-resolver
		downing-provider-class = "akka.cluster.sbr.SplitBrainResolverProvider"
		split-brain-resolver {
			# Select one of the available strategies (see descriptions below):
			# static-quorum, keep-majority, keep-oldest, down-all, lease-majority
			# in a Kubernetes environment the Lease strategy can be a good choice.
			active-strategy = keep-majority
			# Time margin after which shards or singletons that belonged to a downed/removed
			# partition are created in surviving partition. The purpose of this margin is that
			# in case of a network partition the persistent actors in the non-surviving partitions
			# must be stopped before corresponding persistent actors are started somewhere else.
			# This is useful if you implement downing strategies that handle network partitions,
			# e.g. by keeping the larger side of the partition and shutting down the smaller side.
			# Decision is taken by the strategy when there has been no membership or
			# reachability changes for this duration, i.e. the cluster state is stable.
			stable-after = 20s
			# When reachability observations by the failure detector are changed the SBR decisions
			# are deferred until there are no changes within the 'stable-after' duration.
			# If this continues for too long it might be an indication of an unstable system/network
			# and it could result in delayed or conflicting decisions on separate sides of a network
			# partition.
			# As a precaution for that scenario all nodes are downed if no decision is made within
			# `stable-after + down-all-when-unstable` from the first unreachability event.
			# The measurement is reset if all unreachable have been healed, downed or removed, or
			# if there are no changes within `stable-after * 2`.
			# The value can be on, off, or a duration.
			# By default it is 'on' and then it is derived to be 3/4 of stable-after.
			down-all-when-unstable = on
		}
		shutdown-after-unsuccessful-join-seed-nodes = 20s
	}

	coordinated-shutdown {
		terminate-actor-system = on
		exit-jvm = on
		run-by-actor-system-terminate = on
		run-by-jvm-shutdown-hook = on
	}

	projection {
		jdbc {
			# choose one of: mysql-dialect, postgres-dialect, mssql-dialect, oracle-dialect or h2-dialect (testing)
			dialect = "postgres-dialect"
			blocking-jdbc-dispatcher {
				type = Dispatcher
				executor = "thread-pool-executor"
				thread-pool-executor {
					# Use same number of threads as connections in the JDBC connection pool.
					fixed-pool-size = ${jdbc-default.hikari-settings.max-pool-size}
				}
				throughput = 1
			}

			offset-store {
				# set this to your database schema if applicable, empty by default
				schema = ${jdbc-default.schema}
				# the database table name for the offset store
				table = "read_side_offsets"
				# the database table name for the projection management
				management-table = "read_sides"
				# Use lowercase table and column names.
				use-lowercase-schema = true
			}

			debug.verbose-offset-store-logging = false
		}

		# The configuration to use to restart the projection after an underlying streams failure
		# The Akka streams restart source is used to facilitate this behaviour
		# See the streams documentation for more details
		# https://doc.akka.io/docs/akka/current/stream/stream-error.html#delayed-restarts-with-a-backoff-operator
		restart-backoff {
			min-backoff = 3s
			max-backoff = 30s
			random-factor = 0.2

			# -1 will not cap the amount of restarts
			# 0 will disable restarts
			max-restarts = -1
		}

		# The strategy to use to recover from unhandled exceptions without causing the projection to fail
		recovery-strategy {
			# fail - If the first attempt to invoke the handler fails it will immediately give up and fail the stream.
			# skip - If the first attempt to invoke the handler fails it will immediately give up, discard the element and
			# continue with next.
			# retry-and-fail - If the first attempt to invoke the handler fails it will retry invoking the handler with the
			# same envelope this number of `retries` with the `delay` between each attempt. It will give up
			# and fail the stream if all attempts fail.
			# retry-and-skip - If the first attempt to invoke the handler fails it will retry invoking the handler with the
			# same envelope this number of `retries` with the `delay` between each attempt. It will give up,
			# discard the element and continue with next if all attempts fail.
			strategy = fail

			# The number of times to retry handler function
			# This is only applicable to `retry-and-fail` and `retry-and-skip` recovery strategies
			retries = 5

			# The delay between retry attempts
			# Only applicable to `retry-and-fail` and `retry-and-skip` recovery strategies
			retry-delay = 1 s
		}
	}

	# these settings bind Akka management to 0.0.0.0 so that k8s health checks
	# work when running behind istio proxy
	# https://doc.akka.io/docs/akka-management/current/akka-management.html
	management.http.bind-hostname = "0.0.0.0"
  	management.http.bind-port = 8558
}

# general slick configuration
write-side-slick {
	# The Slick profile to use
	# set to one of: slick.jdbc.DerbyProfile$, slick.jdbc.H2Profile$, slick.jdbc.HsqldbProfile$, slick.jdbc.MySQLProfile$,
	# slick.jdbc.PostgresProfile$, slick.jdbc.SQLiteProfile$, slick.jdbc.OracleProfile$
	# profile = <fill this with your profile of choice>
	profile = "slick.jdbc.PostgresProfile$"
	# add here your Slick db settings
	db {
		connectionPool = "HikariCP"
		driver = "org.postgresql.Driver"
		user = ${jdbc-default.user}
		password = ${jdbc-default.password}
		serverName = ${jdbc-default.host}
		portNumber = ${jdbc-default.port}
		databaseName = ${jdbc-default.database}
		url = "jdbc:postgresql://"${write-side-slick.db.serverName}":"${write-side-slick.db.portNumber}"/"${write-side-slick.db.databaseName}"?currentSchema="${jdbc-default.schema}
		# hikariCP settings; see: https://github.com/brettwooldridge/HikariCP
		# Slick will use an async executor with a fixed size queue of 10.000 objects
		# The async executor is a connection pool for asynchronous execution of blocking I/O actions.
		# This is used for the asynchronous query execution API on top of blocking back-ends like JDBC.
		queueSize = 10000 // number of objects that can be queued by the async exector
		# ensures that the database does not get dropped while we are using it
		keepAliveConnection = on
		# See some tips on thread/connection pool sizing on https://github.com/brettwooldridge/HikariCP/wiki/About-Pool-Sizing
		# Keep in mind that the number of threads must equal the maximum number of connections.
		numThreads = 20
		maxConnections = 20
		minConnections = 2

		# This property controls the maximum amount of time that a connection is allowed to sit idle in the pool.
		# Whether a connection is retired as idle or not is subject to a maximum variation of +30 seconds, and average variation
		# of +15 seconds. A connection will never be retired as idle before this timeout. A value of 0 means that idle connections
		# are never removed from the pool. Default: 600000 (10 minutes)
		idleTimeout = 60000 # 60 seconds

		# This property controls the maximum lifetime of a connection in the pool. When a connection reaches this timeout
		# it will be retired from the pool, subject to a maximum variation of +30 seconds. An in-use connection will never be retired,
		# only when it is closed will it then be removed. We strongly recommend setting this value, and it should be at least 30 seconds
		# less than any database-level connection timeout. A value of 0 indicates no maximum lifetime (infinite lifetime),
		# subject of course to the idleTimeout setting. Default: 1800000 (30 minutes)
		maxLifetime = 120000 # 120 seconds
	}
}

jdbc-journal {
	tables {
		# Only used in pre 5.0.0 for backward-compatibility
		# ref: https://github.com/akka/akka-persistence-jdbc/blob/v5.0.0/core/src/main/resources/reference.conf
		legacy_journal {
			tableName = "journal"
			schemaName = ${jdbc-default.schema}
		}

		# this is the new going forward
		# ref: https://github.com/akka/akka-persistence-jdbc/blob/v5.0.0/core/src/main/resources/reference.conf
		event_journal {
			tableName = "event_journal"
			schemaName = ${jdbc-default.schema}
		}

		event_tag {
			tableName = "event_tag"
			schemaName = ${jdbc-default.schema}
		}
	}

	# If you have data from pre 5.0.0 use the legacy akka.persistence.jdbc.journal.dao.legacy.ByteArrayJournalDao
	# Dao. Migration to the new dao will be added in the future.
	dao = "akka.persistence.jdbc.journal.dao.DefaultJournalDao"

	# The size of the buffer used when queueing up events for batch writing. This number must be bigger then the number
	# of events that may be written concurrently. In other words this number must be bigger than the number of persistent
	# actors that are actively peristing at the same time.
	bufferSize = 1000
	# The maximum size of the batches in which journal rows will be inserted
	batchSize = 400
	# The maximum size of the batches in which journal rows will be read when recovering
	replayBatchSize = 400
	# The maximum number of batch-inserts that may be running concurrently
	parallelism = 8

	slick = ${write-side-slick}
}

# the akka-persistence-query provider in use
jdbc-read-journal {
	# New events are retrieved (polled) with this interval.
	refresh-interval = "1s"
	# How many events to fetch in one query (replay) and keep buffered until they
	# are delivered downstreams.
	max-buffer-size = "500"
	tables {
		legacy_journal = ${jdbc-journal.tables.legacy_journal}
		event_journal = ${jdbc-journal.tables.event_journal}
		event_tag = ${jdbc-journal.tables.event_tag}
	}
	slick = ${write-side-slick}
}

# the akka-persistence-snapshot-store in use
jdbc-snapshot-store {
	tables {
		# Only used in pre 5.0.0 for backward-compatibility
		# ref: https://github.com/akka/akka-persistence-jdbc/blob/v5.0.0/core/src/main/resources/reference.conf
		legacy_snapshot {
			tableName = "snapshot"
			schemaName = ${jdbc-default.schema}
		}

		# This is the new configuration going forward
		snapshot {
			tableName = "state_snapshot"
			schemaName = ${jdbc-default.schema}
		}
	}
	slick = ${write-side-slick}
}

# chief of state configuration
chiefofstate {
	# the service name
	service-name = "chiefofstate"
	service-name = ${?COS_SERVICE_NAME}

	# Ask timeout is required to
	# send commands to the aggregate root and receive response
	# the unit value is in second
	ask-timeout = 5
	ask-timeout = ${?COS_COMMAND_HANDLER_TIMEOUT}

	snapshot-criteria {
		# Snapshots are not saved and deleted automatically, events are not deleted
		disable-snapshot = false
		disable-snapshot = ${?COS_DISABLE_SNAPSHOT}
		# Save snapshots automatically every `retention-frequency`
		# Snapshots that have sequence number less than sequence number of the saved snapshot minus `retention-number * retention-frequency` are
		retention-frequency = 100
		retention-frequency = ${?COS_SNAPSHOT_FREQUENCY}
		# number of snapshots to retain
		retention-number = 2
		retention-number = ${?COS_NUM_SNAPSHOTS_TO_RETAIN}
		# this feature allow to clean the journal history
		# Event deletion is triggered after saving a new snapshot. Old events would be deleted prior to old snapshots being deleted
		# reference: https://doc.akka.io/docs/akka/current/typed/persistence-snapshot.html#event-deletion
		delete-events-on-snapshot = false
		delete-events-on-snapshot = ${?COS_JOURNAL_LOGICAL_DELETION}
	}

	grpc {
		client {
			# the deadline timeout, a duration of time after which the RPC times out.
			# it is expressed in milliseconds and it should be less than chiefofstate.ask-timeout
			# Fail to accomplish this will result in timeout exception
			# The default values is 1mn which is really lenient
			deadline-timeout = 60000
			deadline-timeout = ${?COS_GRPC_CALLS_TIMEOUT}
		}
		server {
			port = 9000
			port = ${?COS_PORT}
			address = "0.0.0.0"
			address = ${?COS_ADDRESS}
		}
	}

	write-side {
		# Write handler gRPC host
		host = ""
		host = ${?COS_WRITE_SIDE_HOST}
		# Write handler gRPC port
		port = -1
		port = ${?COS_WRITE_SIDE_PORT}
		# enable TLS on outbound write-handler gRPC calls
		use-tls = false
		use-tls = ${?COS_WRITE_SIDE_USE_TLS}
		# Whether to allow validation of the state and events FQNs
		# if set to true, validation is done, by default it is false.
		enable-protos-validation = false
		enable-protos-validation = ${?COS_WRITE_SIDE_PROTO_VALIDATION}
		# define the fully qualified type url  of the state proto
		# example: chief_of_state.v1.Account
		states-protos = ""
		states-protos = ${?COS_WRITE_SIDE_STATE_PROTOS}
		# list if the fully qualified type url  of the events handled
		# example: "chief_of_state.v1.AccountOpened", "chief_of_state.v1.AccountDebited"
		events-protos = ""
		events-protos = ${?COS_WRITE_SIDE_EVENT_PROTOS}
		# csv of gRPC headers to propagate to write-side handler
		propagated-headers = ""
		propagated-headers = ${?COS_WRITE_SIDE_PROPAGATED_HEADERS}
		# csv of gRPC headers to persist to journal metadtaa
		persisted-headers = ""
		persisted-headers = ${?COS_WRITE_PERSISTED_HEADERS}
	}

	read-side {
		# set this value to true whenever a readSide config is set
		enabled = false
		enabled = ${?COS_READ_SIDE_ENABLED}
	}

	plugin-settings {
		enabled-plugins = ""
		enabled-plugins = ${?COS_ENABLED_PLUGINS}
	}

	telemetry {
		namespace = ""
		namespace = ${?COS_TELEMETRY_NAMESPACE}

		otlp_endpoint = ""
		otlp_endpoint = ${?COS_TELEMETRY_COLLECTOR_ENDPOINT}

		trace_propagators = "b3multi"
		trace_propagators = ${?COS_TRACE_PROPAGATORS}
	}

	# migration configuration
	# This setting are required for a smooth migration to 0.8.0
	migration {
		v1 {
			slick {
				profile = "slick.jdbc.PostgresProfile$"
				# add here your Slick db settings
				db {
					driver = "org.postgresql.Driver"
					user = ${jdbc-default.user}
					user = ${?COS_READ_SIDE_OFFSET_DB_USER}
					password = ${jdbc-default.password}
					password = ${?COS_READ_SIDE_OFFSET_DB_PASSWORD}
					serverName = ${jdbc-default.host}
					serverName = ${?COS_READ_SIDE_OFFSET_DB_HOST}
					portNumber = ${jdbc-default.port}
					portNumber = ${?COS_READ_SIDE_OFFSET_DB_PORT}
					databaseName = ${jdbc-default.database}
					databaseName = ${?COS_READ_SIDE_OFFSET_DB}
					url = "jdbc:postgresql://"${chiefofstate.migration.v1.slick.db.serverName}":"${chiefofstate.migration.v1.slick.db.portNumber}"/"${chiefofstate.migration.v1.slick.db.databaseName}"?currentSchema="${chiefofstate.migration.v1.slick.offset-store.schema}
					connectionPool = "HikariCP"
					keepAliveConnection = false
					# See some tips on thread/connection pool sizing on https://github.com/brettwooldridge/HikariCP/wiki/About-Pool-Sizing
					# Keep in mind that the number of threads must equal the maximum number of connections.
					numThreads = 5
					maxConnections = 5
					minConnections = 0
					idleTimeout = 60000 # 60 seconds
					maxLifetime = 120000 # 120 seconds
				}
				offset-store {
					schema = ${jdbc-default.schema}
					schema = ${?COS_READ_SIDE_OFFSET_DB_SCHEMA}
					table = "read_side_offsets"
					table = ${?COS_READ_SIDE_OFFSET_STORE_TABLE}
					use-lowercase-schema = false
				}
			}
		}
	}
}
