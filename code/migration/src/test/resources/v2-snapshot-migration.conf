jdbc-default {
	host = "0.0.0.0"
	database = "test"
	schema = "cos"
	port = 8004
	user = "test"
	password = "changeme"
}
akka {
	loglevel = ERROR
	log-dead-letters-during-shutdown = on
	log-dead-letters = on
	actor {
		serialize-messages = off
		debug {
			receive = on // log all messages sent to an actor if that actors receive method is a LoggingReceive
			autoreceive = on // log all special messages like Kill, PoisoffPill etc sent to all actors
			lifecycle = on // log all actor lifecycle events of all actors
			fsm = off // enable logging of all events, transitioffs and timers of FSM Actors that extend LoggingFSM
			event-stream = on // enable logging of subscriptions (subscribe/unsubscribe) on the ActorSystem.eventStream
		}
		provider = cluster
		serializers {
			proto = "akka.remote.serialization.ProtobufSerializer"
		}
		serialization-bindings {
			# state is serialized using protobuf
			"scalapb.GeneratedMessage" = proto
			"com.google.protobuf.Message" = proto
		}

		# This will stop the guardian actor in case of any exception which will therefore
		# shutdown the whole actor system
		guardian-supervisor-strategy = "akka.actor.StoppingSupervisorStrategy"
	}

	persistence {
		# akka persistence jdbc
		journal.plugin = "jdbc-journal"
		snapshot-store.plugin = "jdbc-snapshot-store"
	}

	coordinated-shutdown {
		terminate-actor-system = off
		exit-jvm = off
		run-by-actor-system-terminate = off
		run-by-jvm-shutdown-hook = off
	}

	projection {
		slick {
			# The Slick profile to use
			# set to one of: slick.jdbc.DerbyProfile$, slick.jdbc.H2Profile$, slick.jdbc.HsqldbProfile$, slick.jdbc.MySQLProfile$,
			# slick.jdbc.PostgresProfile$, slick.jdbc.SQLiteProfile$, slick.jdbc.OracleProfile$
			# profile = <fill this with your profile of choice>
			profile = "slick.jdbc.PostgresProfile$"
			# add here your Slick db settings
			db {
				driver = "org.postgresql.Driver"
				# default to the journal jdbc settings, but allow optional overrides
				user = ${jdbc-default.user}
				password = ${jdbc-default.password}
				serverName = ${jdbc-default.host}
				portNumber = ${jdbc-default.port}
				databaseName = ${jdbc-default.database}
				url = "jdbc:postgresql://"${akka.projection.slick.db.serverName}":"${akka.projection.slick.db.portNumber}"/"${akka.projection.slick.db.databaseName}"?currentSchema="${akka.projection.slick.offset-store.schema}
				connectionPool = "HikariCP"
				keepAliveConnection = true
			}

			offset-store {
				# set this to your database schema if applicable, empty by default
				schema = ${jdbc-default.schema}
				# the database table name for the offset store
				table = "read_side_offsets"

				# check migration note https://github.com/akka/akka-projection/releases/tag/v1.1.0
				# set this to true this when migration tool is done
				use-lowercase-schema = true
			}
		}
		# The configuration to use to restart the projection after an underlying streams failure
		# The Akka streams restart source is used to facilitate this behaviour
		# See the streams documentation for more details
		# https://doc.akka.io/docs/akka/current/stream/stream-error.html#delayed-restarts-with-a-backoff-operator
		restart-backoff {
			min-backoff = 3s
			max-backoff = 30s
			random-factor = 0.2

			# -1 will not cap the amount of restarts
			# 0 will disable restarts
			max-restarts = -1
		}

		# The strategy to use to recover from unhandled exceptions without causing the projection to fail
		recovery-strategy {
			# fail - If the first attempt to invoke the handler fails it will immediately give up and fail the stream.
			# skip - If the first attempt to invoke the handler fails it will immediately give up, discard the element and
			# continue with next.
			# retry-and-fail - If the first attempt to invoke the handler fails it will retry invoking the handler with the
			# same envelope this number of `retries` with the `delay` between each attempt. It will give up
			# and fail the stream if all attempts fail.
			# retry-and-skip - If the first attempt to invoke the handler fails it will retry invoking the handler with the
			# same envelope this number of `retries` with the `delay` between each attempt. It will give up,
			# discard the element and continue with next if all attempts fail.
			strategy = fail

			# The number of times to retry handler function
			# This is only applicable to `retry-and-fail` and `retry-and-skip` recovery strategies
			retries = 5

			# The delay between retry attempts
			# Only applicable to `retry-and-fail` and `retry-and-skip` recovery strategies
			retry-delay = 1 s
		}
	}
}

# general slick configuration
write-side-slick {
	profile = "slick.jdbc.PostgresProfile$"
	db {
		connectionPool = disabled
		driver = "org.postgresql.Driver"
		user = ${jdbc-default.user}
		password = ${jdbc-default.password}
		serverName = ${jdbc-default.host}
		portNumber = ${jdbc-default.port}
		databaseName = ${jdbc-default.database}
		url = "jdbc:postgresql://"${write-side-slick.db.serverName}":"${write-side-slick.db.portNumber}"/"${write-side-slick.db.databaseName}"?currentSchema="${jdbc-default.schema}
	}
}

jdbc-journal {
	tables {
		# Only used in pre 5.0.0 for backward-compatibility
		# ref: https://github.com/akka/akka-persistence-jdbc/blob/v5.0.0/core/src/main/resources/reference.conf
		legacy_journal {
			tableName = "journal"
			schemaName = ${jdbc-default.schema}
			columnNames {
				ordering = "ordering"
				deleted = "deleted"
				persistenceId = "persistence_id"
				sequenceNumber = "sequence_number"
				created = "created"
				tags = "tags"
				message = "message"
			}
		}

		# this is the new going forward
		# ref: https://github.com/akka/akka-persistence-jdbc/blob/v5.0.0/core/src/main/resources/reference.conf
		event_journal {
			tableName = "event_journal"
			schemaName = ${jdbc-default.schema}
			columnNames {
				ordering = "ordering"
				deleted = "deleted"
				persistenceId = "persistence_id"
				sequenceNumber = "sequence_number"
				writer = "writer"
				writeTimestamp = "write_timestamp"
				adapterManifest = "adapter_manifest"
				eventPayload = "event_payload"
				eventSerId = "event_ser_id"
				eventSerManifest = "event_ser_manifest"
				metaPayload = "meta_payload"
				metaSerId = "meta_ser_id"
				metaSerManifest = "meta_ser_manifest"
			}
		}

		event_tag {
			tableName = "event_tag"
			schemaName = ${jdbc-default.schema}
			columnNames {
				eventId = "event_id"
				tag = "tag"
			}
		}
	}
	tagSeparator = ","
	bufferSize = 1000
	batchSize = 400
	replayBatchSize = 400
	parallelism = 8
	logicalDelete = true
	dao = "akka.persistence.jdbc.journal.dao.DefaultJournalDao"
	slick = ${write-side-slick}
}

# the akka-persistence-query provider in use
jdbc-read-journal {
	# New events are retrieved (polled) with this interval.
	refresh-interval = "1s"
	# How many events to fetch in one query (replay) and keep buffered until they
	# are delivered downstreams.
	max-buffer-size = "500"
	tables {
		legacy_journal = ${jdbc-journal.tables.legacy_journal}
		event_journal = ${jdbc-journal.tables.event_journal}
		event_tag = ${jdbc-journal.tables.event_tag}
	}

	tagSeparator = ","
	# if true, queries will include logically deleted events
	# should not be configured directly, but through property akka-persistence-jdbc.logicalDelete.enable
	# in order to keep consistent behavior over write/read sides
	includeLogicallyDeleted = true

	# Settings for determining if ids (ordering column) in the journal are out of sequence.
	journal-sequence-retrieval {
		# The maximum number of ids that will be retrieved in each batch
		batch-size = 10000
		# In case a number in the sequence is missing, this is the ammount of retries that will be done to see
		# if the number is still found. Note that the time after which a number in the sequence is assumed missing is
		# equal to maxTries * queryDelay
		# (maxTries may not be zero)
		max-tries = 10
		# How often the actor will query for new data
		query-delay = 1 second
		# The maximum backoff time before trying to query again in case of database failures
		max-backoff-query-delay = 1 minute
		# The ask timeout to use when querying the journal sequence actor, the actor should normally repond very quickly,
		# since it always replies with its current internal state
		ask-timeout = 1 second
	}

	dao = "akka.persistence.jdbc.query.dao.DefaultReadJournalDao"
	slick = ${write-side-slick}
}

# the akka-persistence-snapshot-store in use
jdbc-snapshot-store {
	tables {
		# Only used in pre 5.0.0 for backward-compatibility
		# ref: https://github.com/akka/akka-persistence-jdbc/blob/v5.0.0/core/src/main/resources/reference.conf
		legacy_snapshot {
			tableName = "snapshot"
			schemaName = ${jdbc-default.schema}

			columnNames {
				persistenceId = "persistence_id"
				sequenceNumber = "sequence_number"
				created = "created"
				snapshot = "snapshot"
			}
		}

		# This is the new configuration going forward
		snapshot {
			tableName = "state_snapshot"
			schemaName = ${jdbc-default.schema}
			columnNames {
				persistenceId = "persistence_id"
				sequenceNumber = "sequence_number"
				created = "created"

				snapshotPayload = "snapshot_payload"
				snapshotSerId = "snapshot_ser_id"
				snapshotSerManifest = "snapshot_ser_manifest"

				metaPayload = "meta_payload"
				metaSerId = "meta_ser_id"
				metaSerManifest = "meta_ser_manifest"
			}
		}
	}
	dao = "akka.persistence.jdbc.snapshot.dao.DefaultSnapshotDao"

	slick = ${write-side-slick}
}
